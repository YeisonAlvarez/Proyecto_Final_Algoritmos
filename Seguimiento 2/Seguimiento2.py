# -*- coding: utf-8 -*-
# =====================================================
# Universidad del QuindÃ­o - IngenierÃ­a de Sistemas
# Proyecto: AnÃ¡lisis de Algoritmos en el Contexto de la BibliometrÃ­a
# Requerimientos 2, 3 y Seguimiento 2 â€” Similitud, Frecuencia y Grafos
# Autor: Yeison Ãlvarez
# =====================================================

import streamlit as st
import pandas as pd
import google.generativeai as genai
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from sentence_transformers import SentenceTransformer
import textdistance
import re
import json
from pathlib import Path
import chardet
import networkx as nx
import matplotlib.pyplot as plt
from pyvis.network import Network
import tempfile
import os


# =====================================================
# CONFIGURACIÃ“N GENERAL
# =====================================================
st.set_page_config(layout="wide", page_title="AnÃ¡lisis BibliomÃ©trico con IA")
st.title("ğŸ”¬ AnÃ¡lisis BibliomÃ©trico con Inteligencia Artificial")
st.markdown("""
AplicaciÃ³n para los **Requerimientos 2, 3 y Seguimiento 2** del proyecto de AnÃ¡lisis de Algoritmos â€” Universidad del QuindÃ­o.
""")

# =====================================================
# CONFIGURAR API GEMINI
# =====================================================
try:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    gemini_is_configured = True
except (KeyError, AttributeError):
    gemini_is_configured = False
    st.warning("âš ï¸ No se encontrÃ³ la API Key de Google Gemini. Las funciones IA serÃ¡n limitadas.")

# =====================================================
# LISTA DE PALABRAS CLAVE PREDEFINIDAS
# =====================================================
PREDEFINED_KEYWORDS = [
    "Generative models", "Prompting", "Machine learning", "Multimodality",
    "Fine-tuning", "Training data", "Algorithmic bias", "Explainability",
    "Transparency", "Ethics", "Privacy", "Personalization",
    "Human-AI interaction", "AI literacy", "Co-creation"
]

# =====================================================
# FUNCIONES AUXILIARES
# =====================================================
@st.cache_data
def load_data(file_path):
    """Carga CSV detectando automÃ¡ticamente encoding y separador."""
    try:
        with open(file_path, 'rb') as f:
            enc = chardet.detect(f.read())['encoding']
        for sep in [',', ';', '\t']:
            try:
                df = pd.read_csv(file_path, encoding=enc or 'latin1', sep=sep, on_bad_lines='skip', engine='python')
                if len(df.columns) > 1:
                    break
            except Exception:
                continue
        df.columns = df.columns.str.strip().str.lower()
        if not all(col in df.columns for col in ["title", "abstract"]):
            st.error("âŒ El archivo debe contener las columnas: 'title' y 'abstract'.")
            return None
        df = df.dropna(subset=["title", "abstract"])
        st.success(f"âœ… Archivo cargado correctamente con codificaciÃ³n {enc} y separador '{sep}'")
        return df
    except Exception as e:
        st.error(f"âš ï¸ Error al leer el archivo: {e}")
        return None


def preprocess_text(text):
    """Limpieza bÃ¡sica del texto."""
    stopwords = {'a','un','una','el','la','los','las','de','del','en','y','o','con','por','para','su',
                 'the','an','and','in','of','to','is','for','on','with','as','by','this','that','it','are','be'}
    text = text.lower()
    text = re.sub(r'[^a-zÃ¡Ã©Ã­Ã³ÃºÃ¼Ã±\s]', ' ', text)
    tokens = [w.strip() for w in text.split() if len(w) > 2 and w not in stopwords]
    return tokens

# =====================================================
# ALGORITMOS DE SIMILITUD
# =====================================================
def jaccard_similarity(t1, t2):
    s1, s2 = set(preprocess_text(t1)), set(preprocess_text(t2))
    return len(s1 & s2) / len(s1 | s2) if s1 | s2 else 0

def cosine_tfidf(t1, t2):
    v = TfidfVectorizer(tokenizer=preprocess_text, token_pattern=None)
    X = v.fit_transform([t1, t2])
    return cosine_similarity(X[0:1], X[1:2])[0][0]

def levenshtein_sim(t1, t2):
    return textdistance.levenshtein.normalized_similarity(t1, t2)

def euclidean_sim(t1, t2):
    v = TfidfVectorizer(tokenizer=preprocess_text, token_pattern=None)
    X = v.fit_transform([t1, t2])
    dist = euclidean_distances(X[0:1], X[1:2])[0][0]
    return 1 / (1 + dist)

@st.cache_data
def embedding_sim(t1, t2):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    emb = model.encode([t1, t2])
    return cosine_similarity([emb[0]], [emb[1]])[0][0]

def gemini_similarity_analysis(t1, t2):
    """Usa Gemini para analizar similitud conceptual."""
    if not gemini_is_configured:
        return "âš ï¸ Gemini no configurado."
    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
        prompt = f"Compara los siguientes textos y explica su similitud conceptual en 5 lÃ­neas.\n\nTexto 1: {t1}\n\nTexto 2: {t2}"
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Error en Gemini: {e}"

# =====================================================
# REQ 3 â€” FRECUENCIAS Y IA DE PALABRAS NUEVAS
# =====================================================
def predefined_frequencies(df):
    text = " ".join(df['abstract'].str.lower())
    data = []
    for kw in PREDEFINED_KEYWORDS:
        count = len(re.findall(r'\b' + re.escape(kw.lower()) + r'\b', text))
        data.append({'name': kw, 'frequency': count})
    return sorted(data, key=lambda x: x['frequency'], reverse=True)

@st.cache_data
def generate_new_keywords_with_ai(text):
    """Genera nuevas keywords con Gemini."""
    if not gemini_is_configured:
        return {"newKeywords": [], "analysis": "Gemini no configurado."}
    try:
        model = genai.GenerativeModel('gemini-2.5-flash')
        prompt = f"""
        Analiza los siguientes abstracts y propone hasta 15 palabras clave relacionadas con 'Generative AI in Education'.
        Devuelve JSON: {{ "newKeywords":[{{"keyword":"", "frequency":int}}], "analysis":"..." }}
        Textos: {text}
        """
        resp = model.generate_content(prompt, generation_config=genai.types.GenerationConfig(response_mime_type="application/json"))
        return json.loads(resp.text)
    except Exception as e:
        return {"newKeywords": [], "analysis": str(e)}

# =====================================================
# CARGAR DATOS
# =====================================================
DATA_FILE_PATH = Path("DataFinal/unificado.csv")
df_articles = load_data(DATA_FILE_PATH)

if df_articles is not None:
    st.success(f"âœ… {len(df_articles)} artÃ­culos cargados correctamente.")

    tab1, tab2, tab3 = st.tabs(["ğŸ“Š Requerimiento 2: Similitud Textual",
                                "ğŸ“ˆ Requerimiento 3: Frecuencias y Keywords IA",
                                "ğŸ”— Seguimiento 2: Redes BibliomÃ©tricas"])

    # -------------------------------------------------
    # TAB 1 â€” REQUERIMIENTO 2
    # -------------------------------------------------
    with tab1:
        st.header("ğŸ“Š Requerimiento 2 â€” Similitud Textual entre ArtÃ­culos")
        col1, col2 = st.columns(2)
        with col1:
            a1_title = st.selectbox("Primer artÃ­culo:", df_articles['title'])
        with col2:
            a2_title = st.selectbox("Segundo artÃ­culo:", df_articles[df_articles['title'] != a1_title]['title'])

        if st.button("Calcular Similitud", type="primary"):
            t1 = df_articles[df_articles['title'] == a1_title].iloc[0]['abstract']
            t2 = df_articles[df_articles['title'] == a2_title].iloc[0]['abstract']

            jaccard = jaccard_similarity(t1, t2)
            cosine = cosine_tfidf(t1, t2)
            lev = levenshtein_sim(t1, t2)
            eucl = euclidean_sim(t1, t2)
            embed = embedding_sim(t1, t2)
            gem_analysis = gemini_similarity_analysis(t1, t2)

            st.subheader("ğŸ“ˆ Resultados de Similitud")
            colA, colB = st.columns(2)
            with colA:
                st.metric("Jaccard", f"{jaccard:.4f}")
                st.metric("Cosine TF-IDF", f"{cosine:.4f}")
                st.metric("Levenshtein", f"{lev:.4f}")
                st.metric("Euclidiana", f"{eucl:.4f}")
            with colB:
                st.metric("MiniLM (Embeddings IA)", f"{embed:.4f}")
                st.markdown("#### ğŸ¤– Gemini â€” AnÃ¡lisis Conceptual")
                st.info(gem_analysis)

        with st.expander("ğŸ“˜ ExplicaciÃ³n TÃ©cnica de Algoritmos"):
            st.markdown("""
            - **Jaccard:** mide coincidencia de tÃ©rminos (|Aâˆ©B|/|AâˆªB|).  
            - **Cosine TF-IDF:** compara vectores de frecuencia ponderada.  
            - **Levenshtein:** mide distancia de ediciÃ³n carÃ¡cter a carÃ¡cter.  
            - **Euclidiana:** distancia geomÃ©trica entre vectores TF-IDF.  
            - **MiniLM (IA):** embeddings semÃ¡nticos entrenados (Sentence-BERT).  
            - **Gemini (IA):** anÃ¡lisis conceptual generativo del significado textual.
            """)

    # -------------------------------------------------
    # TAB 2 â€” REQUERIMIENTO 3
    # -------------------------------------------------
    with tab2:
        st.header("ğŸ“ˆ Requerimiento 3 â€” Frecuencia y GeneraciÃ³n de Keywords IA")
        freq = predefined_frequencies(df_articles)
        df_freq = pd.DataFrame(freq).set_index('name')
        st.bar_chart(df_freq)

        if st.button("Generar nuevas keywords con IA"):
            corpus = " ".join(df_articles['abstract'].astype(str).tolist())
            ai_result = generate_new_keywords_with_ai(corpus)
            st.session_state['ai_keywords'] = ai_result

        if 'ai_keywords' in st.session_state:
            res = st.session_state['ai_keywords']
            if res.get('newKeywords'):
                df_ai = pd.DataFrame(res['newKeywords'])
                if 'keyword' in df_ai.columns:
                    df_ai = df_ai.rename(columns={'keyword':'name'})
                df_ai = df_ai.set_index('name')
                st.markdown("#### ğŸ” Nuevas palabras IA generadas")
                st.dataframe(df_ai)
                predefined = {k.lower() for k in PREDEFINED_KEYWORDS}
                ai_set = {n.lower() for n in df_ai.index}
                precision = len(predefined & ai_set) / len(ai_set) if ai_set else 0
                st.success(f"**PrecisiÃ³n respecto a palabras originales:** {precision:.2%}")
            else:
                st.warning(f"No se generaron nuevas palabras. Detalle: {res.get('analysis')}")

    # -------------------------------------------------
    # TAB 3 â€” SEGUIMIENTO 2 (Redes)
    # -------------------------------------------------
    with tab3:
        st.header("ğŸ”— Seguimiento 2 â€” Redes de Citaciones y Coocurrencia")
        subtab1, subtab2 = st.tabs(["ğŸ“˜ Red de Citaciones", "ğŸ§© Red de Coocurrencia"])

        # -------------------------------------------------
        # RED DE CITACIONES (grafo dirigido con algoritmos)
        # -------------------------------------------------

        with subtab1:
            st.subheader("ğŸ“˜ Grafo de Citaciones â€” con Caminos MÃ­nimos y Componentes")

            # === Controles de entrada ===
            selected = st.multiselect(
                "Selecciona los artÃ­culos que deseas incluir en el grafo de citaciones:",
                df_articles['title'].tolist(),
                default=[]  # âœ… Ahora inicia vacÃ­o
            )
            threshold = st.slider("Umbral de similitud", 0.0, 1.0, 0.25, 0.05)
            use_emb = st.checkbox("Usar embeddings IA (MiniLM)", value=True)

            # === ConstrucciÃ³n del grafo (solo si se presiona el botÃ³n) ===
            if st.button("Construir Grafo de Citaciones", type="primary"):
                if not selected:
                    st.warning("âš ï¸ Debes seleccionar al menos un artÃ­culo para construir el grafo.")
                else:
                    subset = df_articles[df_articles['title'].isin(selected)]
                    abstracts = subset['abstract'].tolist()
                    titles = subset['title'].tolist()

                    # --- Calcular matriz de similitud ---
                    if use_emb:
                        model = SentenceTransformer('all-MiniLM-L6-v2')
                        emb = model.encode(abstracts, show_progress_bar=True)
                        sim_matrix = cosine_similarity(emb)
                    else:
                        v = TfidfVectorizer(tokenizer=preprocess_text, token_pattern=None)
                        tfidf = v.fit_transform(abstracts)
                        sim_matrix = cosine_similarity(tfidf)

                    # Guardar datos en sesiÃ³n para evitar recalcular
                    st.session_state["grafo_titles"] = titles
                    st.session_state["grafo_matrix"] = sim_matrix
                    st.session_state["grafo_threshold"] = threshold


            # === Mostrar resultados si ya hay un grafo creado ===
            if "grafo_matrix" in st.session_state:
                titles = st.session_state["grafo_titles"]
                sim_matrix = st.session_state["grafo_matrix"]
                threshold = st.session_state["grafo_threshold"]

                df_sim = pd.DataFrame(sim_matrix, index=titles, columns=titles)
                st.markdown("### ğŸ§® Matriz de Similitud")
                st.dataframe(df_sim.style.background_gradient(cmap="Blues").format("{:.2f}"))

                # --- Crear grafo dirigido ---
                G = nx.DiGraph()
                for i, t1 in enumerate(titles):
                    for j, t2 in enumerate(titles):
                        if i != j and sim_matrix[i][j] >= threshold:
                            G.add_edge(t1, t2, weight=round(float(sim_matrix[i][j]), 2))

                if len(G.nodes()) == 0:
                    st.warning("âš ï¸ Sin conexiones, baja el umbral o elige mÃ¡s artÃ­culos.")
                else:
                    # === Grafo con IDs cortos ===
                    id_map = {title: f"A{i+1}" for i, title in enumerate(titles)}
                    G_short = nx.relabel_nodes(G, id_map)

                    fig, ax = plt.subplots(figsize=(10, 8))
                    pos = nx.spring_layout(G_short, k=0.6, seed=42)
                    nx.draw(
                        G_short, pos,
                        with_labels=True, node_color="skyblue", node_size=900,
                        arrows=True, font_size=9, font_weight='bold'
                    )
                    nx.draw_networkx_edge_labels(
                        G_short, pos,
                        edge_labels={(u, v): d['weight'] for u, v, d in G_short.edges(data=True)},
                        font_size=7
                    )
                    plt.title(f"Grafo de Citaciones â€” {len(G.nodes())} nodos / {len(G.edges())} aristas", fontsize=11)
                    st.pyplot(fig)

                    # === Leyenda de artÃ­culos ===
                    st.markdown("### ğŸ—‚ï¸ Leyenda de artÃ­culos")
                    df_leg = pd.DataFrame({"ID": list(id_map.values()), "TÃ­tulo": list(id_map.keys())})
                    st.dataframe(df_leg)

                    # === MÃ©tricas ===
                    densidad = nx.density(G)
                    grado_prom = sum(dict(G.degree()).values()) / len(G)
                    st.markdown(f"**Densidad:** {densidad:.4f} | **Grado promedio:** {grado_prom:.2f}")

                    # === Caminos mÃ­nimos (Dijkstra) ===
                    st.markdown("#### ğŸš€ Caminos MÃ­nimos (Dijkstra)")
                    origen = st.selectbox("Selecciona artÃ­culo origen:", titles, key="origen_dijkstra")
                    destino = st.selectbox("Selecciona artÃ­culo destino:", titles, key="destino_dijkstra")

                    if st.button("Calcular camino mÃ­nimo"):
                        try:
                            path = nx.shortest_path(G, source=origen, target=destino, weight='weight', method='dijkstra')
                            distance = nx.shortest_path_length(G, source=origen, target=destino, weight='weight', method='dijkstra')
                            st.session_state["dijkstra_path"] = path
                            st.session_state["dijkstra_distance"] = distance
                        except nx.NetworkXNoPath:
                            st.session_state["dijkstra_path"] = None
                            st.session_state["dijkstra_distance"] = None

                    if "dijkstra_path" in st.session_state:
                        if st.session_state["dijkstra_path"]:
                            st.success(f"Camino mÃ¡s corto: {' âœ '.join(st.session_state['dijkstra_path'])} (Distancia: {st.session_state['dijkstra_distance']:.2f})")
                        else:
                            st.warning("âš ï¸ No hay camino entre esos nodos.")

                    # === Componentes fuertemente conexas ===
                    st.markdown("#### ğŸ”— Componentes Fuertemente Conexas")
                    comp = list(nx.strongly_connected_components(G))
                    st.write(f"Se detectaron **{len(comp)}** componentes:")
                    for i, c in enumerate(comp, 1):
                        st.write(f"Componente {i}: {', '.join(c)}")


        # -------------------------------------------------
        # RED DE COOCURRENCIA (grafo no dirigido)
        # -------------------------------------------------
        with subtab2:
            st.subheader("ğŸ§© Grafo de Coocurrencia de Palabras Clave")
            window = st.slider("Ventana de palabras", 5, 40, 10, 5)
            threshold = st.slider("Umbral mÃ­nimo de coocurrencia", 1, 10, 2, 1)

            terms = [re.sub(r'[-\s]+', ' ', kw.lower().strip()) for kw in PREDEFINED_KEYWORDS]
            G2 = nx.Graph()
            for t in terms:
                G2.add_node(t)

            for abs_ in df_articles['abstract']:
                tokens = preprocess_text(abs_)
                for i in range(len(tokens)):
                    window_tokens = tokens[i:i+window]
                    presentes = [t for t in terms if any(t in w for w in window_tokens)]
                    for a in range(len(presentes)):
                        for b in range(a+1, len(presentes)):
                            x, y = presentes[a], presentes[b]
                            if G2.has_edge(x, y):
                                G2[x][y]['weight'] += 1
                            else:
                                G2.add_edge(x, y, weight=1)

            edges_validas = [(u, v, d) for u, v, d in G2.edges(data=True) if d['weight'] >= threshold]
            if not edges_validas:
                st.warning("âš ï¸ No se encontraron coocurrencias con ese umbral.")
            else:
                G3 = nx.Graph()
                G3.add_weighted_edges_from([(u, v, d['weight']) for u, v, d in edges_validas])
                fig, ax = plt.subplots(figsize=(9, 7))
                pos = nx.spring_layout(G3, k=0.6, seed=42)
                nx.draw(G3, pos, with_labels=True, node_color="lightgreen", node_size=800, edge_color="gray", font_size=8)
                nx.draw_networkx_edge_labels(G3, pos, edge_labels={(u, v): d['weight'] for u, v, d in G3.edges(data=True)}, font_size=7)
                plt.title(f"Grafo de Coocurrencia â€” {len(G3.nodes())} nodos / {len(G3.edges())} aristas", fontsize=10)
                st.pyplot(fig)

                # ğŸ”¹ Grado de cada nodo
                st.markdown("### ğŸ“Š AnÃ¡lisis del Grafo de Coocurrencia")
                grados = dict(G3.degree())
                df_grado = pd.DataFrame(list(grados.items()), columns=["TÃ©rmino", "Grado"]).sort_values("Grado", ascending=False)
                st.dataframe(df_grado)

                # ğŸ”¹ Componentes conexas
                st.markdown("#### ğŸ”— Componentes Conexas")
                componentes = list(nx.connected_components(G3))
                st.write(f"Se detectaron **{len(componentes)}** componentes:")
                for i, c in enumerate(componentes, 1):
                    st.write(f"Componente {i}: {', '.join(c)}")

                st.success("âœ… AnÃ¡lisis completado correctamente.")
